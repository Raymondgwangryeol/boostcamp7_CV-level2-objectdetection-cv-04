_cfg_dict:
    value:
        auto_scale_lr:
            base_batch_size: 16
            enable: false
        checkpoint_config:
            interval: 1
            max_keep_ckpts: 3
        custom_hooks:
            - type: WandBPrecisionRecallHook
        data:
            samples_per_gpu: 4
            test:
                ann_file: data/coco/annotations/instances_val2017.json
                img_prefix: data/coco/val2017/
                pipeline:
                    - type: LoadImageFromFile
                    - flip: false
                      img_scale:
                        - 1333
                        - 800
                      transforms:
                        - keep_ratio: true
                          type: Resize
                        - type: RandomFlip
                        - mean:
                            - 123.675
                            - 116.28
                            - 103.53
                          std:
                            - 58.395
                            - 57.12
                            - 57.375
                          to_rgb: true
                          type: Normalize
                        - size_divisor: 32
                          type: Pad
                        - keys:
                            - img
                          type: ImageToTensor
                        - keys:
                            - img
                          type: Collect
                      type: MultiScaleFlipAug
                type: CocoDataset
            train:
                ann_file: /data/ephemeral/home/dataset/train.json
                classes:
                    - General trash
                    - Paper
                    - Paper pack
                    - Metal
                    - Glass
                    - Plastic
                    - Styrofoam
                    - Plastic bag
                    - Battery
                    - Clothing
                img_prefix: /data/ephemeral/home/dataset/train
                pipeline:
                    - type: LoadImageFromFile
                    - type: LoadAnnotations
                      with_bbox: true
                    - img_scale:
                        - 4
                        - 4
                      keep_ratio: true
                      type: Resize
                    - flip_ratio: 0.5
                      type: RandomFlip
                    - mean:
                        - 123.675
                        - 116.28
                        - 103.53
                      std:
                        - 58.395
                        - 57.12
                        - 57.375
                      to_rgb: true
                      type: Normalize
                    - size_divisor: 32
                      type: Pad
                    - type: DefaultFormatBundle
                    - keys:
                        - img
                        - gt_bboxes
                        - gt_labels
                      type: Collect
                type: CocoDataset
            val:
                ann_file: /data/ephemeral/home/dataset/val.json
                classes:
                    - General trash
                    - Paper
                    - Paper pack
                    - Metal
                    - Glass
                    - Plastic
                    - Styrofoam
                    - Plastic bag
                    - Battery
                    - Clothing
                img_prefix: /data/ephemeral/home/dataset/val
                pipeline:
                    - type: LoadImageFromFile
                    - flip: false
                      img_scale:
                        - 4
                        - 4
                      transforms:
                        - keep_ratio: true
                          type: Resize
                        - type: RandomFlip
                        - mean:
                            - 123.675
                            - 116.28
                            - 103.53
                          std:
                            - 58.395
                            - 57.12
                            - 57.375
                          to_rgb: true
                          type: Normalize
                        - size_divisor: 32
                          type: Pad
                        - keys:
                            - img
                          type: ImageToTensor
                        - keys:
                            - img
                          type: Collect
                      type: MultiScaleFlipAug
                type: CocoDataset
            workers_per_gpu: 2
        data_root: data/coco/
        dataset_type: CocoDataset
        device: cuda
        dist_params:
            backend: nccl
        evaluation:
            interval: 1
            metric:
                - bbox
            save_best: bbox_mAP
        gpu_ids:
            - 0
        img_norm_cfg:
            mean:
                - 123.675
                - 116.28
                - 103.53
            std:
                - 58.395
                - 57.12
                - 57.375
            to_rgb: true
        load_from: null
        log_config:
            hooks:
                - type: TextLoggerHook
                - init_kwargs:
                    config:
                        backbone: ResNeXt
                        batch_size: 4
                        depth: 101
                        lr: 0.01
                        num_epochs: 12
                    entity: jongseo001111-naver
                    project: retinanet_x101_project
                  type: WandbLoggerHook
            interval: 50
        log_level: INFO
        lr_config:
            min_lr: 1e-05
            policy: CosineAnnealing
            warmup: linear
            warmup_iters: 500
            warmup_ratio: 0.001
        model:
            backbone:
                base_width: 4
                depth: 101
                frozen_stages: 1
                groups: 64
                init_cfg:
                    checkpoint: open-mmlab://resnext101_64x4d
                    type: Pretrained
                norm_cfg:
                    requires_grad: true
                    type: BN
                norm_eval: true
                num_stages: 4
                out_indices:
                    - 0
                    - 1
                    - 2
                    - 3
                style: pytorch
                type: ResNeXt
            bbox_head:
                anchor_generator:
                    octave_base_scale: 4
                    ratios:
                        - 0.5
                        - 1
                        - 2
                    scales_per_octave: 3
                    strides:
                        - 8
                        - 16
                        - 32
                        - 64
                        - 128
                    type: AnchorGenerator
                bbox_coder:
                    target_means:
                        - 0
                        - 0
                        - 0
                        - 0
                    target_stds:
                        - 1
                        - 1
                        - 1
                        - 1
                    type: DeltaXYWHBBoxCoder
                feat_channels: 256
                in_channels: 256
                loss_bbox:
                    loss_weight: 1
                    type: L1Loss
                loss_cls:
                    alpha: 0.25
                    gamma: 2
                    loss_weight: 1
                    type: FocalLoss
                    use_sigmoid: true
                num_classes: 10
                stacked_convs: 4
                type: RetinaHead
            neck:
                add_extra_convs: on_input
                in_channels:
                    - 256
                    - 512
                    - 1024
                    - 2048
                num_outs: 5
                out_channels: 256
                start_level: 1
                type: FPN
            test_cfg:
                max_per_img: 100
                min_bbox_size: 0
                nms:
                    iou_threshold: 0.5
                    type: nms
                nms_pre: 1000
                score_thr: 0.05
            train_cfg:
                allowed_border: -1
                assigner:
                    ignore_iof_thr: -1
                    min_pos_iou: 0
                    neg_iou_thr: 0.4
                    pos_iou_thr: 0.5
                    type: MaxIoUAssigner
                debug: false
                pos_weight: -1
            type: RetinaNet
        mp_start_method: fork
        opencv_num_threads: 0
        optimizer:
            lr: 0.01
            momentum: 0.9
            type: SGD
            weight_decay: 0.0001
        optimizer_config:
            grad_clip:
                max_norm: 35
                norm_type: 2
        resume_from: null
        runner:
            max_epochs: 12
            type: EpochBasedRunner
        seed: 2022
        test_pipeline:
            - type: LoadImageFromFile
            - flip: false
              img_scale:
                - 1333
                - 800
              transforms:
                - keep_ratio: true
                  type: Resize
                - type: RandomFlip
                - mean:
                    - 123.675
                    - 116.28
                    - 103.53
                  std:
                    - 58.395
                    - 57.12
                    - 57.375
                  to_rgb: true
                  type: Normalize
                - size_divisor: 32
                  type: Pad
                - keys:
                    - img
                  type: ImageToTensor
                - keys:
                    - img
                  type: Collect
              type: MultiScaleFlipAug
        train_pipeline:
            - type: LoadImageFromFile
            - type: LoadAnnotations
              with_bbox: true
            - img_scale:
                - 1333
                - 800
              keep_ratio: true
              type: Resize
            - flip_ratio: 0.5
              type: RandomFlip
            - mean:
                - 123.675
                - 116.28
                - 103.53
              std:
                - 58.395
                - 57.12
                - 57.375
              to_rgb: true
              type: Normalize
            - size_divisor: 32
              type: Pad
            - type: DefaultFormatBundle
            - keys:
                - img
                - gt_bboxes
                - gt_labels
              type: Collect
        val_dataloader:
            samples_per_gpu: 1
            shuffle: false
            workers_per_gpu: 2
        work_dir: /data/ephemeral/home/level2-objectdetection-cv-04/mmdetection/workdir_retinanet_x101_64x4d_fpn_1x_coco.py
        workflow:
            - - train
              - 1
_filename:
    value: /data/ephemeral/home/level2-objectdetection-cv-04/mmdetection/configs/retinanet/retinanet_x101_64x4d_fpn_1x_coco.py
_text:
    value: "/data/ephemeral/home/level2-objectdetection-cv-04/mmdetection/configs/_base_/models/retinanet_r50_fpn.py\n# model settings\nmodel = dict(\n    type='RetinaNet',\n    backbone=dict(\n        type='ResNet',\n        depth=50,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        norm_cfg=dict(type='BN', requires_grad=True),\n        norm_eval=True,\n        style='pytorch',\n        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50')),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        start_level=1,\n        add_extra_convs='on_input',\n        num_outs=5),\n    bbox_head=dict(\n        type='RetinaHead',\n        num_classes=80,\n        in_channels=256,\n        stacked_convs=4,\n        feat_channels=256,\n        anchor_generator=dict(\n            type='AnchorGenerator',\n            octave_base_scale=4,\n            scales_per_octave=3,\n            ratios=[0.5, 1.0, 2.0],\n            strides=[8, 16, 32, 64, 128]),\n        bbox_coder=dict(\n            type='DeltaXYWHBBoxCoder',\n            target_means=[.0, .0, .0, .0],\n            target_stds=[1.0, 1.0, 1.0, 1.0]),\n        loss_cls=dict(\n            type='FocalLoss',\n            use_sigmoid=True,\n            gamma=2.0,\n            alpha=0.25,\n            loss_weight=1.0),\n        loss_bbox=dict(type='L1Loss', loss_weight=1.0)),\n    # model training and testing settings\n    train_cfg=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.5,\n            neg_iou_thr=0.4,\n            min_pos_iou=0,\n            ignore_iof_thr=-1),\n        allowed_border=-1,\n        pos_weight=-1,\n        debug=False),\n    test_cfg=dict(\n        nms_pre=1000,\n        min_bbox_size=0,\n        score_thr=0.05,\n        nms=dict(type='nms', iou_threshold=0.5),\n        max_per_img=100))\n\n/data/ephemeral/home/level2-objectdetection-cv-04/mmdetection/configs/_base_/datasets/coco_detection.py\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ntrain_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(type='LoadAnnotations', with_bbox=True),\n    dict(type='Resize', img_scale=(1333, 800), keep_ratio=True),\n    dict(type='RandomFlip', flip_ratio=0.5),\n    dict(type='Normalize', **img_norm_cfg),\n    dict(type='Pad', size_divisor=32),\n    dict(type='DefaultFormatBundle'),\n    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels']),\n]\ntest_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(\n        type='MultiScaleFlipAug',\n        img_scale=(1333, 800),\n        flip=False,\n        transforms=[\n            dict(type='Resize', keep_ratio=True),\n            dict(type='RandomFlip'),\n            dict(type='Normalize', **img_norm_cfg),\n            dict(type='Pad', size_divisor=32),\n            dict(type='ImageToTensor', keys=['img']),\n            dict(type='Collect', keys=['img']),\n        ])\n]\ndata = dict(\n    samples_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        pipeline=train_pipeline),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        pipeline=test_pipeline),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        pipeline=test_pipeline))\nevaluation = dict(interval=1, metric='bbox')\n\n/data/ephemeral/home/level2-objectdetection-cv-04/mmdetection/configs/_base_/schedules/schedule_1x.py\n# optimizer\noptimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=None)\n# learning policy\n# lr_config = dict(\n#     policy='step',\n#     warmup='linear',\n#     warmup_iters=500,\n#     warmup_ratio=0.001,\n#     step=[8, 11])\n\n#lr Cosine Annealing with linear warmup으로 변경\nlr_config = dict(\n    policy='CosineAnnealing',\n    min_lr=0.00001,\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=0.001)\n\nrunner = dict(type='EpochBasedRunner', max_epochs=12)\n\n/data/ephemeral/home/level2-objectdetection-cv-04/mmdetection/configs/_base_/default_runtime.py\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\ncustom_hooks = [dict(type='NumClassCheckHook')]\n\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n\n# disable opencv multithreading to avoid system being overloaded\nopencv_num_threads = 0\n# set multi-process start method as `fork` to speed up the training\nmp_start_method = 'fork'\n\n# Default setting for scaling LR automatically\n#   - `enable` means enable scaling LR automatically\n#       or not by default.\n#   - `base_batch_size` = (8 GPUs) x (2 samples per GPU).\nauto_scale_lr = dict(enable=False, base_batch_size=16)\n\n/data/ephemeral/home/level2-objectdetection-cv-04/mmdetection/configs/retinanet/retinanet_r50_fpn_1x_coco.py\n_base_ = [\n    '../_base_/models/retinanet_r50_fpn.py',\n    '../_base_/datasets/coco_detection.py',\n    '../_base_/schedules/schedule_1x.py', '../_base_/default_runtime.py'\n]\n# optimizer\noptimizer = dict(type='SGD', lr=0.01, momentum=0.9, weight_decay=0.0001)\n\n/data/ephemeral/home/level2-objectdetection-cv-04/mmdetection/configs/retinanet/retinanet_x101_64x4d_fpn_1x_coco.py\n_base_ = './retinanet_r50_fpn_1x_coco.py'\n\nmodel = dict(\n    backbone=dict(\n        type='ResNeXt',\n        depth=101,\n        groups=64,\n        base_width=4,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        norm_cfg=dict(type='BN', requires_grad=True),\n        style='pytorch',\n        init_cfg=dict(\n            type='Pretrained', checkpoint='open-mmlab://resnext101_64x4d'))\n)\n\n# Add the log_config to integrate WandB logging\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        dict(\n            type='WandbLoggerHook',\n            init_kwargs=dict(\n                project='retinanet_x101_project',\n                entity='jongseo001111-naver',  # Replace with your WandB username\n                config=dict(\n                    lr = 0.01, \n                    batch_size = 4,  \n                    num_epochs = 12,  \n                    backbone ='ResNeXt',\n                    depth = 101\n                )\n            )\n        )\n    ]\n)\n\nevaluation = dict(\n    interval=1,  # Frequency of evaluation (e.g., every epoch)\n    metric=['bbox'],  # Metrics to evaluate, e.g., 'bbox' for object detection\n    save_best='bbox_mAP'  # Save the checkpoint with the best mAP\n)\n\ncustom_hooks = [\n    dict(\n        type='WandBPrecisionRecallHook',\n    )\n]\n\nval_dataloader = dict(samples_per_gpu=1, workers_per_gpu=2, shuffle=False)\n"
_wandb:
    value:
        cli_version: 0.18.3
        m: []
        python_version: 3.10.13
        t:
            "1":
                - 1
                - 5
                - 37
                - 38
                - 41
                - 53
                - 55
                - 80
            "2":
                - 1
                - 5
                - 37
                - 38
                - 41
                - 53
                - 55
                - 80
            "3":
                - 16
                - 23
                - 24
                - 55
                - 61
            "4": 3.10.13
            "5": 0.18.3
            "8":
                - 5
            "12": 0.18.3
            "13": linux-x86_64
